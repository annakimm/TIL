{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "dcda5386710f49f5956055f3366ddb33"
   },
   "source": [
    "# 엔트로피"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "dd2ee70864344475bf5be8df53e6c445"
   },
   "source": [
    "## 엔트로피의 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "ad233b36c05f4f89af62977750268f76"
   },
   "source": [
    "$Y=0$ 또는 $Y=1$ 인 두 가지 값을 가지는 확률 분포가 다음과 같이 세 종류가 있다고 하자.\n",
    "\n",
    "* 확률 분포 $Y_1$: $P(Y=0)=0.5$, $P(Y=1)=0.5$\n",
    "* 확률 분포 $Y_2$: $P(Y=0)=0.8$, $P(Y=1)=0.2$\n",
    "* 확률 분포 $Y_3$: $P(Y=0)=1.0$, $P(Y=1)=0.0$\n",
    "\n",
    "이 확률 값이 베이지안 확률이라면 확률 분포 $Y_1$은 $y$값에 대해 아무것도 모르는 상태, $Y_3$은 $y$값이 0이라고 100% 확신하는 상태, $Y_2$은 $y$값이 0이라고 믿지만 아닐 수도 있다는 것을 아는 상태를 나타내고 있을 것이다. **확률 분포들이 가지는 이런 확신의 정도를 수치로 표현하는 것을 엔트로피(entropy)**라고 한다. 동일하게 $y=0$이라는 결론을 내리더라고 $y_1$보다 $y_3$을 더 확실하게 0이라고 말할 수 있다. 확률 변수의 여러가지 값이 나올 확률이 대부분 비슷한 경우에는 엔트로피가 높아진다. 반대로 특정한 값이 나올 확률이 높아지고 나머지 값의 확률은 낮아진다면 엔트로피가 작아진다. \n",
    "\n",
    "물리학에서는 상태가 분산되어 있는 정도를 엔트로피로 정의한다. 여러가지로 고루 분산되어 있을 수 있으면 엔트로피가 높고 특정한 하나의 상태로 몰려있으면 엔트로피가 낮다. 확률분포의 엔트로피는 물리학의 엔트로피 용어를 빌려온 것이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "dac37abae2e84890a2da69e73603abcc"
   },
   "source": [
    "엔트로피는 수학적으로 다음과 같이 정의한다.\n",
    "\n",
    "확률변수 $Y$가 베르누이나 카테고리 분포와 같은 이산 확률변수이면\n",
    "\n",
    "$$ H[Y] = -\\sum_{k=1}^K p(y_k) \\log_2 p(y_k) $$\n",
    "\n",
    "이 식에서 $K$는 $X$가 가질 수 있는 클래스의 수이고 $p(y)$는 확률질량함수이다.\n",
    "\n",
    "확률변수 $Y$가 연속 확률변수이면\n",
    "\n",
    "$$ H[Y] = -\\int p(y) \\log_2 p(y) \\; dy $$\n",
    "\n",
    "이 식에서 $p(y)$는 확률밀도함수이다.\n",
    "\n",
    "로그의 밑(base)이 2로 정의된 것은 정보통신과 관련을 가지는 역사적인 이유때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "school_cell_uuid": "c0884b33c1fc4904b6dadfbfee38a5f4"
   },
   "source": [
    "위에서 예를 든 $Y_1$, $Y_1$, $Y_1$에 대해 엔트로피는 구하면 다음과 같다.\n",
    "\n",
    "$$ H[Y_1] = -\\dfrac{1}{2} \\log_2 \\dfrac{1}{2} -\\dfrac{1}{2} \\log_2 \\dfrac{1}{2} = 1 $$\n",
    "\n",
    "$$ H[Y_2] = -\\dfrac{8}{10} \\log_2 \\dfrac{8}{10} -\\dfrac{2}{10} \\log_2 \\dfrac{2}{10} = 0.72 $$\n",
    "\n",
    "$$ H[Y_3] = -1 \\log_2 1 -0 \\log_2 0 = 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "school_cell_uuid": "e3604a72cf8a4df59f2781b8a5cb01d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-0.5 * np.log2(0.5) - 0.5 * np.log2(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "school_cell_uuid": "8efb9991d35644e1bebe98c3dca40d01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7219280948873623"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-0.8 * np.log2(0.8) - 0.2 * np.log2(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "school_cell_uuid": "0ca84628b0364fe89c0502acf05fbd03"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1546319456101628e-14"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps = np.finfo(float).eps # log안에 0을 쓰면 에러가 나므로 아주 작은 값 eps를 넣음\n",
    "-1 * np.log2(1) - eps * np.log2(eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "school_cell_uuid": "0bcbfce1f8aa430aaf587e9fef2168d5"
   },
   "source": [
    "엔트로피 계산에서 $p(y)=0$인 경우에는 다음과 같은 극한값을 사용한다.\n",
    "\n",
    "$$ \\lim_{p\\rightarrow 0} \\; p\\log_2{p} = 0 $$\n",
    "\n",
    "이 값은 로피탈의 정리(l'Hôpital's rule)에서 구할 수 있다. $p$가 0에 가까워지는 게 $log_2{p}$가 $-\\infty$에 가까워지는 것보다 빠르다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "5a7a5c303a0e47ed927583d8a0bb7c56"
   },
   "source": [
    "## 엔트로피의 성질"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "2ec0485f00c14f5f8edae45987747c7d"
   },
   "source": [
    "만약 확률분포가 결정론적이면 즉, 특정한 하나의 값이 나올 확률이 1이고 나머지 값은 나올 수 없다면 엔트로피는 0이며 이 값이 가장 작은 엔트로피 값이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "f1d641944f1e400389cc259257120121"
   },
   "source": [
    "반대로 엔트로피의 최대값은 이산 확률변수의 클래스의 갯수에 따라 달라진다. 만약 이산 확률변수가 가질 수 있는 클래스가 $2^K$개이고 **이산 확률변수가 가질 수 있는 엔트로피의 최대값은 각 클래스가 모두 같은 확률을 가지는 때이다.** 이 때 엔트로피의 값은\n",
    "\n",
    "$$ H = -\\frac{2^K}{2^K}\\log_2\\dfrac{1}{2^K} = K $$\n",
    "\n",
    "이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "d1de5931d6454f12a5725a21555c59b6"
   },
   "source": [
    "## 엔트로피와 정보량"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "b7070bfcfb7d49fcb8e0f93161892e38"
   },
   "source": [
    "**엔트로피는 확률변수가 담을 수 있는 정보량**을 표시한다. 확률변수가 담을 수 있는 정보량이란 **확률변수의 표본값을 관측해서 얻을 수 있는 정보의 종류**를 말한다. 엔트로피가 0이면 확률변수는 결정론적이므로 확률 변수의 표본값이 변화하지 않는다. 따라서 확률 변수의 표본값을 관측한다고 해도 우리가 얻을 수 있는 추가 정보는 없다. 엔트로피가 크다면 확률변수의 표본값이 가질 수 있는 실질적인 경우의 수가 증가하므로 표본값을 실제로 관측하기 전까지는 아는 것이 없다. 즉 확률변수의 표본값이 우리에게 가져다 줄 수 있는 정보의 량이 많다는 뜻이다.\n",
    "\n",
    "엔트로피는 원래 통신 분야에서 데이터가 가지고 있는 정보량을 계산하기 위해 고안되었다. 예를 4개의 알파벳 A, B, C, D 로 씌여진 문서가 있다고 하자. 이 문서를 0과 1로 이루어진 이진수로 변환할 때 일반적인 경우라면 다음과 같이 인코딩을 할 것이다.\n",
    "\n",
    "* A = \"00\"\n",
    "* B = \"01\"\n",
    "* C = \"10\"\n",
    "* D = \"11\"\n",
    "\n",
    "이렇게 인코딩을 하면 1,000 글자로 이루어진 문서는 이진수 2,000개가 된다.\n",
    "\n",
    "만약 문서에서 각 알파벳이 나올 확률이 동일하지 않고 다음과 같다고 가정하자.\n",
    "\n",
    "$$ \\Big\\{ \\dfrac{1}{2}, \\dfrac{1}{4}, \\dfrac{1}{8}, \\dfrac{1}{8} \\Big\\} $$\n",
    "\n",
    "이 때는 다음과 같이 가변길이 인코딩(variable length encoding)을 하면 인코딩된 이진수의 수를 줄일 수 있다. A가 나올 확률이 가장 높으므로 A의 인코딩을 가장 간단하게 한다. B를 1로 인코딩하면 신호가 연속으로 전달될 때 C, D와 구별할 수 없어 디코딩(신호를 원래 기호로 바꿔주는 것)할 수 없게 된다.\n",
    "\n",
    "* A = \"0\"\n",
    "* B = \"10\"\n",
    "* C = \"110\"\n",
    "* D = \"111\"\n",
    "\n",
    "인코딩된 이진수의 숫자는 다음 계산에서 약 1,750개가 됨을 알 수 있다. 보내야 하는 신호의 수가 250/2000이 감소하고, 엔트로피는 2가 아니라 1.75만 보내면 거기에 포함된 정보를 모두 알수 있다는 것을 의미한다.\n",
    "\n",
    "$$ \n",
    "\\left(1000 \\times \\dfrac{1}{2}\\right) \\cdot 1 + \n",
    "\\left(1000 \\times \\dfrac{1}{4}\\right) \\cdot 2 + \n",
    "\\left(1000 \\times \\dfrac{1}{8}\\right) \\cdot 3 + \n",
    "\\left(1000 \\times \\dfrac{1}{8}\\right) \\cdot 3 = 1750\n",
    "$$ \n",
    "\n",
    "1.75는 알파벳 한 글자를 인코딩하는데 필요한 평균 비트(bit)수이며 확률변수의 엔트로피 값과 같다.\n",
    "\n",
    "$$ H = -\\dfrac{1}{2}\\log_2\\dfrac{1}{2} -\\dfrac{1}{4}\\log_2\\dfrac{1}{4} -\\dfrac{2}{8}\\log_2\\dfrac{1}{8} = 1.75 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "school_cell_uuid": "4e6202693db94194ac80e9e61865c20e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.75"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1 / 2 * np.log2(1 / 2) - 1 / 4 * np.log2(1 / 4) - 2 / 8 * np.log2(1 / 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "bootstrap": {
     "panel": {
      "class": "panel-default"
     }
    },
    "school_cell_uuid": "c6751c7d6c594780b12d309f3a06a230"
   },
   "source": [
    "##### 연습 문제 1\n",
    "\n",
    "A, B, C, D, E, F, G, H의 8글자로 이루어진 문서가 있고 각각의 글자가 나올 확률이 다음과 같다고 가정하자.\n",
    "\n",
    "$$ \\Big\\{ \\dfrac{1}{2}, \\dfrac{1}{4}, \\dfrac{1}{8}, \\dfrac{1}{16}, \\dfrac{1}{64}, \\dfrac{1}{64}, \\dfrac{1}{64}, \\dfrac{1}{64} \\Big\\} $$\n",
    "\n",
    "이 문서를 위한 가변 길이 인코딩 방식을 서술하고 한 글자를 인코딩하는데 필요한 평균 비트수를 계산하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_y = -1/2*np.log2(1/2)-1/4*np.log2(1/4)-1/8*np.log2(1/8)-1/16*np.log2(1/16)-4/64*np.log2(1/64)\n",
    "H_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = 0\n",
    "B = 10\n",
    "C = 110\n",
    "D = 1110\n",
    "E = 111100\n",
    "F = 111101\n",
    "G = 111110\n",
    "H = 111111"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "26310bb9e861444f85a04ac009cc92f3"
   },
   "source": [
    "## 표본 데이터가 주어진 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "effb48d4b3194a72a9b805a3e1524e4b"
   },
   "source": [
    "확률 변수 모형, 즉 이론적인 확률 밀도(질량) 함수가 아닌 실제 데이터가 주어진 경우에는 확률질량함수를 추정하여 엔트로피를 계산한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "f84e4e55208a42fea74b5cec2e109740"
   },
   "source": [
    "예를 들어 데이터가 모두 80개가 있고 그 중 Y = 0 인 데이터가 40개, Y = 1인 데이터가 40개 있는 경우는 엔트로피가 1이다.\n",
    "\n",
    "$$ P(y=0) = \\dfrac{40}{80} = \\dfrac{1}{2} $$\n",
    "\n",
    "$$ P(y=1) = \\dfrac{40}{80} = \\dfrac{1}{2} $$\n",
    "\n",
    "$$ \n",
    "H[Y] = -\\dfrac{1}{2}\\log_2\\left(\\dfrac{1}{2}\\right) -\\dfrac{1}{2}\\log_2\\left(\\dfrac{1}{2}\\right) = \\dfrac{1}{2} + \\dfrac{1}{2}  = 1 \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "school_cell_uuid": "798bae70f8d641bb81b4e3e77ebf6203"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "- 1 / 2 * np.log2(1 / 2) - 1 / 2 * np.log2(1 / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "5bf020f65a554c5ea9c4acd2c8e9e089"
   },
   "source": [
    "만약 데이터가 모두 60개가 있고 그 중 Y= 0 인 데이터가 20개, Y = 1인 데이터가 40개 있는 경우는 엔트로피가 약 0.92이다.\n",
    "\n",
    "$$ P(y=0) = \\dfrac{20}{60} = \\dfrac{1}{3} $$\n",
    "\n",
    "$$ P(y=1) = \\dfrac{40}{60} = \\dfrac{2}{3} $$\n",
    "\n",
    "$$ H[Y] = -\\dfrac{1}{3}\\log_2\\left(\\dfrac{1}{3}\\right) -\\dfrac{2}{3}\\log_2\\left(\\dfrac{2}{3}\\right) = 0.92 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "school_cell_uuid": "3d286ab650aa42cf828db15d3e632726"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9182958340544896"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1 / 3 * np.log2(1 / 3) - 2 / 3 * np.log2(2 / 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "1c15de02be3e47a69ca9eaf7a3b8fc3d"
   },
   "source": [
    "만약 데이터가 모두 40개가 있고 그 중 Y= 0 인 데이터가 30개, Y = 1인 데이터가 10개 있는 경우는 엔트로피가 약 0.81이다.\n",
    "\n",
    "$$ P(y=0) = \\dfrac{30}{40} = \\dfrac{3}{4} $$\n",
    "\n",
    "$$ P(y=1) = \\dfrac{10}{40} = \\dfrac{1}{4} $$\n",
    "\n",
    "$$ H[Y] = -\\dfrac{3}{4}\\log_2\\left(\\dfrac{3}{4}\\right) -\\dfrac{1}{4}\\log_2\\left(\\dfrac{1}{4}\\right) = 0.81 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "school_cell_uuid": "d3521caa1559431b9e4e4658ee769e95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8112781244591328"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-3 / 4 * np.log2(3 / 4) - 1 / 4 * np.log2(1 / 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "7f8bb6fca21e42dd85f554dc284bf6e4"
   },
   "source": [
    "만약 데이터가 모두 20개가 있고 그 중 Y= 0 인 데이터가 20개, Y = 1인 데이터가 0개 있는 경우는 엔트로피가 0이다.\n",
    "\n",
    "$$ P(y=0) = \\dfrac{20}{20} = 1 $$\n",
    "\n",
    "$$ P(y=1) = \\dfrac{0}{20} = 0 $$\n",
    "\n",
    "$$ H[Y] = -1\\log_2\\left(1\\right) - 0\\log_2\\left(0\\right) = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "391b9a5dd27c4664a80e857a2b312dbc"
   },
   "source": [
    "## 조건부 엔트로피"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "494db9c62eb045d199f25aa8513c20e4"
   },
   "source": [
    "조건부 엔트로피는 **상관관계가 있는 두 확률변수 $X$, $Y$가 있고 $X$의 값을 안다면 $Y$의 확률변수가 가질 수 있는 정보의 양**을 뜻한다. 즉, 다른 관련있는 확률변수의 값을 안다면 각 $x$일때 $y$를 얼마나 잘 분류할 수 있는지 엔트로피로 표현할 수 있다. 수학적으로는 다음과 같이 정의한다.\n",
    "\n",
    "$$ H[Y \\mid X] = - \\sum_i \\sum_j \\,p(x_i, y_j) \\log_2 p(y_j \\mid x_i)  $$\n",
    "\n",
    "$$ H[Y \\mid X] = -\\int \\int p(x, y) \\log_2 p(y \\mid x) \\; dxdy $$\n",
    "\n",
    "\n",
    "이 식은 조건부 확률 분포의 정의를 사용하여 다음과 같이 고칠 수 있다.\n",
    "\n",
    "$$ H[Y \\mid X]  = \\sum_i \\,p(x_i)\\,H[Y \\mid x_i] $$\n",
    "\n",
    "$$ H[Y \\mid X]  = \\int p(x)\\,H[Y \\mid x] \\; dx $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "d3ce0fb280964d0b87758a303fd3fb8a"
   },
   "source": [
    "이 식은 이산 확률변수에 대해서는 다음과 같이 증명할 수 있다.\n",
    "\n",
    "$$ \n",
    "\\begin{eqnarray}\n",
    "H[Y \\mid X] \n",
    "&=& - \\sum_i \\sum_j \\,p(x_i, y_j) \\log_2 p(y_j \\mid x_i) \\\\\n",
    "&=& - \\sum_i \\sum_j p(y_j \\mid x_i) p(x_i) \\log_2 p(y_j \\mid x_i) \\\\\n",
    "&=& - \\sum_i p(x_i) \\sum_j p(y_j \\mid x_i)  \\log_2 p(y_j \\mid x_i) \\\\\n",
    "&=& \\sum_i p(x_i) H[Y \\mid x_i] \\\\\n",
    "\\end{eqnarray}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "530cb492fad94375862d0ac4c9d8086c"
   },
   "source": [
    "조건부 엔트로피의 개념을 스팸 메일 분류 문제를 통해 알아보자. 스팸 메일 분류 모형을 만들기 위한 메일 데이터가 80개가 있다 이 중 40개가 정상 메일($Y=0$), 40개가 스팸 메일($Y=1$)이다.\n",
    "\n",
    "스팸 메일 여부를 특정 키워드가 존재하는지($X=1$) 혹은 존재하지 않는지($X=0$)의 여부로 알아보고자 한다. 키워드 후보로는 $X_1$, $X_2$, $X_3$ 세가지가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "d4ee5676a26741beb3f6d52482bc6457"
   },
   "source": [
    "예를 들어  $X_1$, $Y$ 값의 관계가 다음과 같다고 하자.\n",
    "\n",
    "| | $Y = 0$ | $Y = 1$ | |\n",
    "|-|-|-|-|\n",
    "| $X_1 = 0$ | 30 | 10 | 40 |\n",
    "| $X_1 = 1$ | 10 | 30 | 40 |\n",
    "|          | 40 | 40 | 80 |\n",
    "\n",
    "이 때 조건부 엔트로피는 다음과 같이 계산된다.\n",
    "\n",
    "$$ \n",
    "\\begin{eqnarray}\n",
    "H[Y \\mid X_1 ] \n",
    "&=& p(X_1=0)\\,H[Y \\mid X_1=0] + p(X_1=1)\\,H[Y \\mid X_1=1] \\\\\n",
    "&=& \\dfrac{40}{80} \\cdot 0.81 + \\dfrac{40}{80} \\cdot 0.81 = 0.81 \\\\\n",
    "&:& x_1\\text{키워드로 분류하였을 때 y값 나올 확률의 엔트로피}\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다른 확률변수 $X$의 값을 모르고 $y$만으로 분류문제를 풀 때 $P(y=1) = 0.5$이고 $H[y] = 1$ 이었는데 $x=0$ 일 때 $P(y=1) = 1/4$, $x=1$ 일 때 $P(y=1) = 3/4$으로 확률이 변했으므로 $x_1$은 $y$가 스팸이 아닌지(0), 스팸인지(1)을 구별하는데 영향(도움)을 준 확률변수이고, 얼마나 도움이 되었는지는 엔트로피로 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "b69df2b7713947de9ecb3f82250943e6"
   },
   "source": [
    " $X_2$, $Y$ 값의 관계는 다음과 같다.\n",
    "\n",
    "| | $Y = 0$ | $Y = 1$ |  |\n",
    "|-|-|-|-|\n",
    "| $X_2 = 0$ | 20 | 40 | 60 |\n",
    "| $X_2 = 1$ | 20 | 0  | 20 |\n",
    "|          | 40 | 40 | 80 |\n",
    "\n",
    "조건부 엔트로피의 값은 다음과 같다.\n",
    "\n",
    "$$ \n",
    "\\begin{eqnarray}\n",
    "H[Y \\mid X_2 ] \n",
    "&=& p(X_2=0)\\,H[Y \\mid X_2=0] + p(X_2=1)\\,H[Y \\mid X_2=1] \\\\\n",
    "&=& \\dfrac{60}{80} \\cdot 0.92 + \\dfrac{20}{80} \\cdot 0 = 0.69\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "6d1973e9b9cb495ba8a56e2f759c614d"
   },
   "source": [
    "만약  $X_3$, $Y$ 값이 다음과 같다면\n",
    "\n",
    "| | $Y = 0$ | $Y = 1$ |  |\n",
    "|-|-|-|-|\n",
    "| $X_3 = 0$ | 0 | 40 | 40 |\n",
    "| $X_3 = 1$ | 40 | 0  | 40 |\n",
    "|          | 40 | 40 | 80 |\n",
    "\n",
    "조건부 엔트로피의 값은 0이 된다.\n",
    "\n",
    "$$ \n",
    "\\begin{eqnarray}\n",
    "H[Y \\mid X_3 ] \n",
    "&=& p(X_3=0)\\,H[Y \\mid X_3=0] + p(X_2=1)\\,H[Y \\mid X_3=1]  = 0\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_2, x_3$도 $y$를 분류하는데 도움이 되는 변수이다. 가장 효과있는 확률변수는 $y$의 조건부 엔트로피를 가장 작게 만드는 확률변수이다($x_3$). 하나의 확률변수만 사용해 분류문제를 푼다면 그 확률변수를 분석에 사용하면 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "617eacd7999a4b07bf35c90c097dd30e"
   },
   "source": [
    "위의 정의와 예제에서 조건부 엔트로피는 $X$의 값에 의해 만들어지는 새로운 $Y$ 확률분포의 가중평균임을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "25b7cb9166ce4c619c9dc2a665e270b7"
   },
   "source": [
    "## 크로스 엔트로피"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "3039df3df952494285c6abb0c3830d98"
   },
   "source": [
    "두 확률분포 $p(y)$, $q(y)$의 크로스 엔트로피(cross entropy) $H[p,q]$은 다음과 같의 정의한다.\n",
    "\n",
    "$$ H[p,q] = -\\sum_{k=1}^K p(y_k) \\log_2 q(y_k) $$\n",
    "\n",
    "$$ H[p,q] = -\\int p(y) \\log_2 q(y) dy $$\n",
    "\n",
    "**크로스 엔트로피는 주로 분류 문제의 목표값 분포와 예측값 분포가 얼마나 닮았는지 비교하는데 사용된다.**\n",
    "\n",
    "예를 들어 이진 분류에서는 $Y$은 0 또는 1이라는 값만 가질 수 있다. 또 예측값 $\\hat{y}$은 $Y=1$일 확률 $\\theta$라고 하자.\n",
    "모수 $\\theta$로 대표되는 $Y$의 분포와 모수 $\\hat\\theta$로 대표되는 $\\hat{Y}$의 분포라고 하면 크로스 엔트로피는 다음처럼 추정할 수 있다.\n",
    "\n",
    "$$ \n",
    "\\begin{eqnarray}\n",
    "H[Y, \\hat{Y}] \n",
    "&=& -\\theta \\log_2 \\hat{\\theta} - (1 - \\theta) \\log_2 (1 - \\hat{\\theta}) \\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "이 값은 $\\hat\\theta = \\theta$일 때 최소값이 된다.\n",
    "\n",
    "데이터로부터 크로스 엔트로피의 추정치를 구하려면 다음 식을 이용한다.\n",
    "$$ \n",
    "\\begin{eqnarray}\n",
    "H[Y, \\hat{Y}] \n",
    "&\\approx& -\\dfrac{1}{N} \\sum_{i=1}^N \\left( y_i \\log_2 \\hat{y}_i + (1 - y_i) \\log_2 (1 - \\hat{y}_i) \\right) \\\\\n",
    "\\end{eqnarray}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **분류문제에서는 accuracy와 크로스 엔트로피를 error function으로 사용해 모델 정확도를 측정한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "fde14e3b71994fd9b7809fa3f8013a2d"
   },
   "source": [
    "## 쿨백-라이블러 발산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "2c1b914fa49b4296a6bc4b753f4f99ae"
   },
   "source": [
    "쿨백-라이블러 발산(Kullback-Leibler divergence)은 두 확률분포 $p(y)$, $q(y)$의 차이를 정량화하는 방법의 하나이다. 다음과 같이 정의한다. \n",
    "\n",
    "$$ \n",
    "\\begin{eqnarray}\n",
    "KL(p || q)  \n",
    "&=& H[p, q] - H[p]\n",
    "&=& \\int p(y) \\log_2 \\left(\\dfrac{p(y)}{q(y)}\\right) dy\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "쿨백-라이블러 발산은 크로스 엔트로피에서 대상이 되는 분포의 엔트로피는 뺀 값이므로 상대 엔트로피(relative entropy)라고도 한다.\n",
    "그 값은 항상 양수이며 두 확률분포 $p(x)$, $q(x)$가 완전히 같을 경우에만 0이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "d8c3f3eb2ac942a398a739a082b3ca7e"
   },
   "source": [
    "## 지니 불순도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "57415ae21dfa4a58986c6561889b68b4"
   },
   "source": [
    "엔트로피와 유사한 개념으로 지니 불순도(Gini impurity)라는 것이 있다. 지니 불순도는 엔트로피처럼 확률 분포가 어느 쪽에 치우쳐있는가를 재는 척도지만 로그를 사용하지 않으므로 계산량이 더 적어 엔트로피 대신으로 많이 사용된다.\n",
    "\n",
    "$$ G[Y] = \\sum_{k=1}^K p(y_k) (1 - p(y_k)) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "엔트로피와 지니불순도 둘다 $P(y_k) = 1$ 이면 결과값이 0이다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
